{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Word2vec - Word Embeddings"
      ],
      "metadata": {
        "id": "NEy_EiGwcu_f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install adjustText --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYEeA3s6dLN5",
        "outputId": "91dbf5a3-b326-47c3-a01b-3ee057f363ac"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Building wheel for adjustText (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KVeoQspVcnG7"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "from sklearn.manifold import TSNE\n",
        "from adjustText import adjust_text"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Understanding the data**\n",
        "## Downloading the data"
      ],
      "metadata": {
        "id": "ucCkWo4ceNMa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'http://mlg.ucd.ie/files/datasets/bbc-fulltext.zip'\n",
        "\n",
        "def download_data(url, data_dir):\n",
        "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
        "\n",
        "  os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "  file_path = os.path.join(data_dir, 'bbc-fulltext.zip')\n",
        "\n",
        "  if not os.path.exists(file_path):\n",
        "    print('Downloading file...')\n",
        "    filename, _ = urlretrieve(url, file_path)\n",
        "  else:\n",
        "    print(\"File already exists\")\n",
        "\n",
        "  extract_path = os.path.join(data_dir, 'bbc')\n",
        "\n",
        "  if not os.path.exists(extract_path):\n",
        "    with zipfile.ZipFile(os.path.join(data_dir, 'bbc-fulltext.zip'), 'r') as zipf:\n",
        "      zipf.extractall(data_dir)\n",
        "  else:\n",
        "    print(\"bbc-fulltext.zip has already been extracted\")\n",
        "\n",
        "\n",
        "download_data(url, 'data')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vuImF5Mhc7A6",
        "outputId": "3b692acc-0aa2-4a33-fb74-3a519ca2e1d5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading file...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read Data without Preprocessing\n",
        "\n",
        "Reads data as it is to a string and tokenize it using spaces and returns a list of words\n"
      ],
      "metadata": {
        "id": "7Kxvb7_iiO5-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_data(data_dir):\n",
        "    \n",
        "    # This will contain the full list of stories\n",
        "    news_stories = []\n",
        "    \n",
        "    print(\"Reading files\")\n",
        "    \n",
        "    i = 0 # Just used for printing progress\n",
        "    for root, dirs, files in os.walk(data_dir):\n",
        "        \n",
        "        for fi, f in enumerate(files):\n",
        "            \n",
        "            # We don't read the readme file\n",
        "            if 'README' in f:\n",
        "                continue\n",
        "            \n",
        "            # Printing progress\n",
        "            i += 1\n",
        "            print(\".\"*i, f, end='\\r')\n",
        "            \n",
        "            # Open the file\n",
        "            with open(os.path.join(root, f), encoding='latin-1') as f:\n",
        "                \n",
        "                story = []\n",
        "                # Read all the lines\n",
        "                for row in f:\n",
        "                                        \n",
        "                    story.append(row.strip())\n",
        "                    \n",
        "                # Create a single string with all the rows in the doc\n",
        "                story = ' '.join(story)                        \n",
        "                # Add that to the list\n",
        "                news_stories.append(story)  \n",
        "                \n",
        "        print('', end='\\r')\n",
        "        \n",
        "    print(f\"\\nDetected {len(news_stories)} stories\")\n",
        "    return news_stories\n",
        "                \n",
        "  \n",
        "news_stories = read_data(os.path.join('data', 'bbc'))\n",
        "\n",
        "# Printing some stats and sample data\n",
        "print(f\"{sum([len(story.split(' ')) for story in news_stories])} words found in the total news set\")\n",
        "print('Example words (start): ',news_stories[0][:50])\n",
        "print('Example words (end): ',news_stories[-1][-50:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZv80_jAiPkv",
        "outputId": "ce192adb-47f9-4efc-cc7b-84cb7128e00a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading files\n",
            "................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................. 163.txt\n",
            "Detected 2225 stories\n",
            "865163 words found in the total news set\n",
            "Example words (start):  Real will finish abandoned match  Real Madrid and \n",
            "Example words (end):  nt\" and they cannot be copied for commercial gain.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Build a Tokenizer**"
      ],
      "metadata": {
        "id": "C428587GjKe-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(\n",
        "    num_words=None,\n",
        "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
        "    lower=True,\n",
        "    split=' ',\n",
        ")\n",
        "\n",
        "tokenizer.fit_on_texts(news_stories)\n",
        "print(\"Data fitted on the tokenizer\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLuYmx6RiYGB",
        "outputId": "d1d87abb-f583-4d15-c87c-8c63d8a39fb7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data fitted on the tokenizer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploring the tokenizer"
      ],
      "metadata": {
        "id": "6GXMDjezmevu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_vocab = len(tokenizer.word_index.items()) + 1\n",
        "print(f\"Vocabulary size: {n_vocab}\")\n",
        "\n",
        "print(\"\\nWords at the top\")\n",
        "print('\\t', dict(list(tokenizer.word_index.items())[:10]))\n",
        "print(\"\\nWords at the bottom\")\n",
        "print('\\t', dict(list(tokenizer.word_index.items())[-10:]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6I0o0kEQkB25",
        "outputId": "fadfd9d0-d1da-4f91-91d5-f885db17a2e5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 32360\n",
            "\n",
            "Words at the top\n",
            "\t {'the': 1, 'to': 2, 'of': 3, 'and': 4, 'a': 5, 'in': 6, 'for': 7, 'is': 8, 'that': 9, 'on': 10}\n",
            "\n",
            "Words at the bottom\n",
            "\t {'shefrin': 32350, 'holly': 32351, 'frankin': 32352, 'bloopers': 32353, \"tabloids'\": 32354, 'scrapbook': 32355, 'souvenir': 32356, 'stepdaughter': 32357, 'ass': 32358, 'saver': 32359}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build a Tokenizer (Refined)\n",
        "\n",
        "Here, we will restrict the vocabulary to 15000 and eleminate words except the first most common 15000 words\n"
      ],
      "metadata": {
        "id": "zhzaGOU7nud5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "\n",
        "n_vocab = 15000 + 1\n",
        "tokenizer = Tokenizer(\n",
        "    num_words=n_vocab-1,\n",
        "    lower=True,\n",
        "    split=' ',\n",
        "    oov_token=''\n",
        ")\n",
        "\n",
        "tokenizer.fit_on_texts(news_stories)\n",
        "print(\"Data fitted on the tokenizer\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ua0OPWZQnCyy",
        "outputId": "5c455861-2b9d-4113-b1e4-0a420cc60733"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data fitted on the tokenizer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9QQZ2Bqwopmp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}