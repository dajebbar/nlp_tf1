{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Word2vec - Word Embeddings"
      ],
      "metadata": {
        "id": "NEy_EiGwcu_f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install adjustText --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYEeA3s6dLN5",
        "outputId": "91dbf5a3-b326-47c3-a01b-3ee057f363ac"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Building wheel for adjustText (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KVeoQspVcnG7"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "from sklearn.manifold import TSNE\n",
        "from adjustText import adjust_text"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Understanding the data**\n",
        "## Downloading the data"
      ],
      "metadata": {
        "id": "ucCkWo4ceNMa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'http://mlg.ucd.ie/files/datasets/bbc-fulltext.zip'\n",
        "\n",
        "def download_data(url, data_dir):\n",
        "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
        "\n",
        "  os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "  file_path = os.path.join(data_dir, 'bbc-fulltext.zip')\n",
        "\n",
        "  if not os.path.exists(file_path):\n",
        "    print('Downloading file...')\n",
        "    filename, _ = urlretrieve(url, file_path)\n",
        "  else:\n",
        "    print(\"File already exists\")\n",
        "\n",
        "  extract_path = os.path.join(data_dir, 'bbc')\n",
        "\n",
        "  if not os.path.exists(extract_path):\n",
        "    with zipfile.ZipFile(os.path.join(data_dir, 'bbc-fulltext.zip'), 'r') as zipf:\n",
        "      zipf.extractall(data_dir)\n",
        "  else:\n",
        "    print(\"bbc-fulltext.zip has already been extracted\")\n",
        "\n",
        "\n",
        "download_data(url, 'data')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vuImF5Mhc7A6",
        "outputId": "3b692acc-0aa2-4a33-fb74-3a519ca2e1d5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading file...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read Data without Preprocessing\n",
        "\n",
        "Reads data as it is to a string and tokenize it using spaces and returns a list of words\n"
      ],
      "metadata": {
        "id": "7Kxvb7_iiO5-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_data(data_dir):\n",
        "    \n",
        "    # This will contain the full list of stories\n",
        "    news_stories = []\n",
        "    \n",
        "    print(\"Reading files\")\n",
        "    \n",
        "    i = 0 # Just used for printing progress\n",
        "    for root, dirs, files in os.walk(data_dir):\n",
        "        \n",
        "        for fi, f in enumerate(files):\n",
        "            \n",
        "            # We don't read the readme file\n",
        "            if 'README' in f:\n",
        "                continue\n",
        "            \n",
        "            # Printing progress\n",
        "            i += 1\n",
        "            print(\".\"*i, f, end='\\r')\n",
        "            \n",
        "            # Open the file\n",
        "            with open(os.path.join(root, f), encoding='latin-1') as f:\n",
        "                \n",
        "                story = []\n",
        "                # Read all the lines\n",
        "                for row in f:\n",
        "                                        \n",
        "                    story.append(row.strip())\n",
        "                    \n",
        "                # Create a single string with all the rows in the doc\n",
        "                story = ' '.join(story)                        \n",
        "                # Add that to the list\n",
        "                news_stories.append(story)  \n",
        "                \n",
        "        print('', end='\\r')\n",
        "        \n",
        "    print(f\"\\nDetected {len(news_stories)} stories\")\n",
        "    return news_stories\n",
        "                \n",
        "  \n",
        "news_stories = read_data(os.path.join('data', 'bbc'))\n",
        "\n",
        "# Printing some stats and sample data\n",
        "print(f\"{sum([len(story.split(' ')) for story in news_stories])} words found in the total news set\")\n",
        "print('Example words (start): ',news_stories[0][:50])\n",
        "print('Example words (end): ',news_stories[-1][-50:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZv80_jAiPkv",
        "outputId": "ce192adb-47f9-4efc-cc7b-84cb7128e00a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading files\n",
            "................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................. 163.txt\n",
            "Detected 2225 stories\n",
            "865163 words found in the total news set\n",
            "Example words (start):  Real will finish abandoned match  Real Madrid and \n",
            "Example words (end):  nt\" and they cannot be copied for commercial gain.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Build a Tokenizer**"
      ],
      "metadata": {
        "id": "C428587GjKe-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(\n",
        "    num_words=None,\n",
        "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
        "    lower=True,\n",
        "    split=' ',\n",
        ")\n",
        "\n",
        "tokenizer.fit_on_texts(news_stories)\n",
        "print(\"Data fitted on the tokenizer\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLuYmx6RiYGB",
        "outputId": "d1d87abb-f583-4d15-c87c-8c63d8a39fb7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data fitted on the tokenizer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploring the tokenizer"
      ],
      "metadata": {
        "id": "6GXMDjezmevu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_vocab = len(tokenizer.word_index.items()) + 1\n",
        "print(f\"Vocabulary size: {n_vocab}\")\n",
        "\n",
        "print(\"\\nWords at the top\")\n",
        "print('\\t', dict(list(tokenizer.word_index.items())[:10]))\n",
        "print(\"\\nWords at the bottom\")\n",
        "print('\\t', dict(list(tokenizer.word_index.items())[-10:]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6I0o0kEQkB25",
        "outputId": "fadfd9d0-d1da-4f91-91d5-f885db17a2e5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 32360\n",
            "\n",
            "Words at the top\n",
            "\t {'the': 1, 'to': 2, 'of': 3, 'and': 4, 'a': 5, 'in': 6, 'for': 7, 'is': 8, 'that': 9, 'on': 10}\n",
            "\n",
            "Words at the bottom\n",
            "\t {'shefrin': 32350, 'holly': 32351, 'frankin': 32352, 'bloopers': 32353, \"tabloids'\": 32354, 'scrapbook': 32355, 'souvenir': 32356, 'stepdaughter': 32357, 'ass': 32358, 'saver': 32359}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build a Tokenizer (Refined)\n",
        "\n",
        "Here, we will restrict the vocabulary to 15000 and eleminate words except the first most common 15000 words\n"
      ],
      "metadata": {
        "id": "zhzaGOU7nud5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "\n",
        "n_vocab = 15000 + 1\n",
        "tokenizer = Tokenizer(\n",
        "    num_words=n_vocab-1,\n",
        "    lower=True,\n",
        "    split=' ',\n",
        "    oov_token=''\n",
        ")\n",
        "\n",
        "tokenizer.fit_on_texts(news_stories)\n",
        "print(\"Data fitted on the tokenizer\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ua0OPWZQnCyy",
        "outputId": "5c455861-2b9d-4113-b1e4-0a420cc60733"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data fitted on the tokenizer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checking the results of the tokenizer"
      ],
      "metadata": {
        "id": "N4m8zZ7ypt_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Original: {news_stories[0][:100]}\")\n",
        "print(f\"Sequence IDs: {tokenizer.texts_to_sequences([news_stories[0][:100]])[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATQVrRsXpuzB",
        "outputId": "71cf4795-b650-44ca-fc38-24d87acf58f7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: Real will finish abandoned match  Real Madrid and Real Socieded will play the final six minutes of t\n",
            "Sequence IDs: [286, 23, 1881, 4501, 328, 286, 1076, 5, 286, 1, 23, 153, 2, 253, 191, 495, 4, 1360]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Converting all articles to word ID sequences**"
      ],
      "metadata": {
        "id": "ClBK_N0bqfAc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "news_sequences = tokenizer.texts_to_sequences(news_stories)"
      ],
      "metadata": {
        "id": "bqV5PZ2Pp78_"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Generating skip-grams from the corpus**\n",
        "\n",
        "In TensorFlow you have the convenient `tf.keras.preprocessing.sequence.skipgrams()` function to generate skipgrams.\n"
      ],
      "metadata": {
        "id": "hEDZmb2PuCOP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "news_sequences[0][:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqbRiNQnqkXe",
        "outputId": "9469d870-7ed6-44b8-cda8-939fbe65711c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[286, 23, 1881, 4501, 328]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_word_ids = news_sequences[0][:5]\n",
        "sample_phrase = ' '.join([tokenizer.index_word[wid] for wid in sample_word_ids])\n",
        "print(f\"Sample phrase: {sample_phrase}\")\n",
        "print(f\"Sample word IDs: {sample_word_ids}\\n\")\n",
        "\n",
        "window_size = 1 # How many words to consider left and right.\n",
        "\n",
        "inputs, labels = tf.keras.preprocessing.sequence.skipgrams(\n",
        "    sample_word_ids, \n",
        "    vocabulary_size=n_vocab, \n",
        "    window_size=window_size, negative_samples=1.0, shuffle=False,\n",
        "    categorical=False, sampling_table=None, seed=None\n",
        ")\n",
        "\n",
        "\n",
        "print(\"Sample skip-grams\")\n",
        "\n",
        "for inp, lbl in zip(inputs, labels):\n",
        "    print(f\"\\tInput: {inp} ({[tokenizer.index_word[wi] for wi in inp]}) / Label: {lbl}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9BAUE6dvP53",
        "outputId": "d7202e8c-f412-4827-9ebe-602cda7c2d49"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample phrase: real will finish abandoned match\n",
            "Sample word IDs: [286, 23, 1881, 4501, 328]\n",
            "\n",
            "Sample skip-grams\n",
            "\tInput: [286, 23] (['real', 'will']) / Label: 1\n",
            "\tInput: [23, 286] (['will', 'real']) / Label: 1\n",
            "\tInput: [23, 1881] (['will', 'finish']) / Label: 1\n",
            "\tInput: [1881, 23] (['finish', 'will']) / Label: 1\n",
            "\tInput: [1881, 4501] (['finish', 'abandoned']) / Label: 1\n",
            "\tInput: [4501, 1881] (['abandoned', 'finish']) / Label: 1\n",
            "\tInput: [4501, 328] (['abandoned', 'match']) / Label: 1\n",
            "\tInput: [328, 4501] (['match', 'abandoned']) / Label: 1\n",
            "\tInput: [4501, 13729] (['abandoned', \"cameroon's\"]) / Label: 0\n",
            "\tInput: [1881, 5840] (['finish', 'housewives']) / Label: 0\n",
            "\tInput: [23, 7357] (['will', 'advances']) / Label: 0\n",
            "\tInput: [1881, 13328] (['finish', 'barnes']) / Label: 0\n",
            "\tInput: [328, 6533] (['match', 'betamax']) / Label: 0\n",
            "\tInput: [286, 6115] (['real', 'sells']) / Label: 0\n",
            "\tInput: [4501, 9146] (['abandoned', 'ginepri']) / Label: 0\n",
            "\tInput: [23, 11943] (['will', 'discovering']) / Label: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Generating negative candidates\n",
        "\n",
        "Word2vec algorithms rely on negative candidates to understand words that do not appear in the context of a given target word.\n"
      ],
      "metadata": {
        "id": "aWGjunZWyI3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs, labels = tf.keras.preprocessing.sequence.skipgrams(\n",
        "    sample_word_ids, \n",
        "    vocabulary_size=len(tokenizer.word_index.items())+1, \n",
        "    window_size=window_size, negative_samples=0, shuffle=False,    \n",
        ")\n",
        "\n",
        "inputs, labels = np.array(inputs), np.array(labels)\n",
        "\n",
        "negative_sampling_candidates, true_expected_count, sampled_expected_count = tf.random.log_uniform_candidate_sampler(\n",
        "    # A true context word that appears in the context of the target\n",
        "    true_classes=inputs[:1,1:], # [b, 1] sized tensor\n",
        "    num_true=1, # number of true words per example\n",
        "    num_sampled=10,\n",
        "    unique=True,\n",
        "    range_max=n_vocab,            \n",
        "    name=\"negative_sampling\"\n",
        ")\n",
        "\n",
        "print(f\"Positive sample: {inputs[:1,1:]}\")\n",
        "print(f\"Negative samples: {negative_sampling_candidates}\")\n",
        "print(f\"true_expected_count: {true_expected_count}\")\n",
        "print(f\"sampled_expected_count: {sampled_expected_count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVittDQMv6SB",
        "outputId": "afc2520f-85fc-4684-ffad-eecf531d9385"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive sample: [[23]]\n",
            "Negative samples: [  44  832  819  226    1   32    9 2025   23    0]\n",
            "true_expected_count: [[0.04571897]]\n",
            "sampled_expected_count: [0.02485704 0.00137159 0.00139331 0.00501681 0.37742305 0.03362463\n",
            " 0.1037828  0.00056434 0.04571897 0.5608634 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`Using tf.nn.sampled_softmax_loss()`**"
      ],
      "metadata": {
        "id": "wC-S8g7fzzHL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(n_vocab, sampling_factor=1e-05)\n",
        "\n",
        "print(sampling_table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaQMC9bPy39v",
        "outputId": "145b3ebb-c2b4-492e-9e78-02f95b192551"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.00315225 0.00315225 0.00547597 ... 1.         1.         1.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating data (positive + negative candidates)"
      ],
      "metadata": {
        "id": "AEIB9azG0BMH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def skip_gram_data_generator(sequences, window_size, batch_size, negative_samples, vocab_size, seed=None):\n",
        "    \n",
        "    rand_sequence_ids = np.arange(len(sequences))                    \n",
        "    np.random.shuffle(rand_sequence_ids)\n",
        "\n",
        "\n",
        "    for si in rand_sequence_ids:\n",
        "        \n",
        "        positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
        "            sequences[si], \n",
        "            vocabulary_size=vocab_size, \n",
        "            window_size=window_size, \n",
        "            negative_samples=0.0, \n",
        "            shuffle=False,\n",
        "            sampling_table=sampling_table,\n",
        "            seed=seed\n",
        "        )\n",
        "        \n",
        "        targets, contexts, labels = [], [], []\n",
        "        \n",
        "        for target_word, context_word in positive_skip_grams:\n",
        "            context_class = tf.expand_dims(tf.constant([context_word], dtype=\"int64\"), 1)\n",
        "            \n",
        "            negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
        "              true_classes=context_class,\n",
        "              num_true=1,\n",
        "              num_sampled=negative_samples,\n",
        "              unique=True,\n",
        "              range_max=vocab_size,              \n",
        "              name=\"negative_sampling\")\n",
        "\n",
        "            # Build context and label vectors (for one target word)\n",
        "            context = tf.concat(\n",
        "                [tf.constant([context_word], dtype='int64'), negative_sampling_candidates],\n",
        "                axis=0\n",
        "            )\n",
        "            \n",
        "            label = tf.constant([1] + [0]*negative_samples, dtype=\"int64\")\n",
        "\n",
        "            # Append each element from the training example to global lists.\n",
        "            targets.extend([target_word]*(negative_samples+1))\n",
        "            contexts.append(context)\n",
        "            labels.append(label)\n",
        "\n",
        "        contexts, targets, labels = np.concatenate(contexts), np.array(targets), np.concatenate(labels)\n",
        "        \n",
        "        assert contexts.shape[0] == targets.shape[0]\n",
        "        assert contexts.shape[0] == labels.shape[0]\n",
        "        \n",
        "        # If seed is not provided generate a random one\n",
        "        if not seed:\n",
        "            seed = random.randint(0, 10e6)\n",
        "            \n",
        "        np.random.seed(seed)\n",
        "        np.random.shuffle(contexts)\n",
        "        np.random.seed(seed)\n",
        "        np.random.shuffle(targets)\n",
        "        np.random.seed(seed)\n",
        "        np.random.shuffle(labels)\n",
        "        \n",
        "    \n",
        "        for eg_id_start in range(0, contexts.shape[0], batch_size):            \n",
        "            yield (\n",
        "                targets[eg_id_start: min(eg_id_start+batch_size, targets.shape[0])], \n",
        "                contexts[eg_id_start: min(eg_id_start+batch_size, contexts.shape[0])]\n",
        "            ), labels[eg_id_start: min(eg_id_start+batch_size, labels.shape[0])]\n",
        "\n",
        "\n",
        "news_skip_gram_gen = skip_gram_data_generator(\n",
        "    news_sequences, 4, 10, 5, n_vocab\n",
        ")\n",
        "\n",
        "for btc, bl in news_skip_gram_gen:\n",
        "    \n",
        "    print(btc)\n",
        "    print(bl)\n",
        "    \n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Oa_5hqbz6fv",
        "outputId": "c098c306-4a2a-467e-eb27-76cb15697cc1"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(array([12058, 12058,    54,    54,   242,  3124,  4353,  3272,  1839,\n",
            "         249]), array([   1,  130,  188,  461, 1046,   42,   69,    0,  313, 1522]))\n",
            "[0 0 0 1 0 0 0 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mRpW7Ox10ImR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}