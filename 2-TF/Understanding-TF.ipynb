{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "70f70684",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73b4766",
   "metadata": {},
   "source": [
    "# Sigmoid example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dde6b87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def layer(x, W, b):\n",
    "    # building the graph\n",
    "    return tf.math.sigmoid(tf.matmul(x, W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e49383dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]], dtype=np.float32)\n",
    "\n",
    "init_w = tf.initializers.RandomUniform(minval=-.1, maxval=.1)(shape=[10,5])\n",
    "W = tf.Variable(init_w, dtype=tf.float32, name='W')\n",
    "\n",
    "init_b = tf.initializers.RandomUniform()(shape=[5])\n",
    "b = tf.Variable(init_b, dtype=tf.float32, name='b')\n",
    "\n",
    "h = layer(x, W, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc1a828a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h: [[0.5018517  0.49334908 0.50805944 0.44643217 0.46883595]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"h: {h.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfaafec6",
   "metadata": {},
   "source": [
    "# Defining Inputs\n",
    "3 different way to feed data to tf program:\n",
    "- as np array\n",
    "- as tf tensors\n",
    "- using `tf.data` **API** to create an input *pipeline*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa390795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feeding data as tensor\n",
    "x = tf.constant(\n",
    "    [[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]], \n",
    "    dtype=tf.float32,\n",
    "    name=\"x\"\n",
    ")\n",
    "\n",
    "init_w = tf.initializers.RandomUniform(minval=-.1, maxval=.1)(shape=[10,5])\n",
    "W = tf.Variable(init_w, dtype=tf.float32, name='W')\n",
    "\n",
    "init_b = tf.initializers.RandomUniform()(shape=[5])\n",
    "b = tf.Variable(init_b, dtype=tf.float32, name='b')\n",
    "\n",
    "h = layer(x, W, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba3138f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h: [[0.44630384 0.49760556 0.48088944 0.5210194  0.5187529 ]]\n",
      "type: <class 'tensorflow.python.framework.ops.EagerTensor'>\n"
     ]
    }
   ],
   "source": [
    "print(f\"h: {h.numpy()}\")\n",
    "print(f\"type: {type(h)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d47ba984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.data api\n",
    "filenames = [f\"./iris.data.{i}\" for i in range(1,4)]\n",
    "\n",
    "dataset = tf.data.experimental.CsvDataset(\n",
    "    filenames,\n",
    "    [\n",
    "        tf.float32,\n",
    "        tf.float32,\n",
    "        tf.float32,\n",
    "        tf.float32,\n",
    "        tf.string,\n",
    "    ]\n",
    ")\n",
    "\n",
    "# organize our data into inputs and labels\n",
    "dataset = dataset.map(lambda x1,x2,x3,x4,y: (tf.stack([x1,x2,x3,x4]), y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "48decd3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(corrupted) X => [ 5.8 -4.   1.2  0.2]\tY => Iris-setosa\n",
      "(corrupted) X => [ 4.6  3.6 -1.   0.2]\tY => Iris-setosa\n",
      "(corrupted) X => [ 4.7 -3.2  1.6  0.2]\tY => Iris-setosa\n",
      "(corrupted) X => [-5.   3.5  1.3  0.3]\tY => Iris-setosa\n",
      "(corrupted) X => [-5.1  3.8  1.6  0.2]\tY => Iris-setosa\n",
      "(corrupted) X => [ 6.9 -3.1  4.9  1.5]\tY => Iris-versicolor\n",
      "(corrupted) X => [-5.2  2.7  3.9  1.4]\tY => Iris-versicolor\n",
      "(corrupted) X => [ 5.8  2.7  4.1 -1. ]\tY => Iris-versicolor\n",
      "(corrupted) X => [ 6.1 -2.8  4.7  1.2]\tY => Iris-versicolor\n",
      "(corrupted) X => [ 5.4  3.  -4.5  1.5]\tY => Iris-versicolor\n",
      "(corrupted) X => [ 7.1 -3.   5.9  2.1]\tY => Iris-virginica\n",
      "(corrupted) X => [ 5.7  2.5  5.  -2. ]\tY => Iris-virginica\n",
      "(corrupted) X => [-6.   2.2  5.   1.5]\tY => Iris-virginica\n",
      "(corrupted) X => [ 7.2 -3.2  6.   1.8]\tY => Iris-virginica\n",
      "(corrupted) X => [ 6.1  2.6  5.6 -1.4]\tY => Iris-virginica\n"
     ]
    }
   ],
   "source": [
    "for next_element in dataset:\n",
    "    x, y = next_element[0].numpy(), next_element[1].numpy().decode(\"ascii\")\n",
    "    if np.min(x) < .0:\n",
    "        print(f\"(corrupted) X => {x}\\tY => {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "35baf73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.filter(lambda x, y: tf.reduce_min(x) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a7c6638d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape = (5, 4), y.shape = (5,)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 5\n",
    "dataset = dataset.batch(batch_size=batch_size)\n",
    "for next_element in dataset:\n",
    "    x, y = next_element[0].numpy(), next_element[1].numpy()    \n",
    "    print(f\"x.shape = {x.shape}, y.shape = {y.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d8a224",
   "metadata": {},
   "source": [
    "# Mnist classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cdbb7bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 7s 1us/step\n",
      "11501568/11490434 [==============================] - 7s 1us/step\n",
      "(Before) x_train has shape: (60000, 28, 28)\n",
      "(Before) x_test has shape: (10000, 28, 28)\n",
      "(Before) x_train has - min: 0, max: 255\n",
      "(Before) x_test has - min: 0, max: 255\n",
      "(After) x_train has shape: (60000, 784)\n",
      "(After) x_test has shape: (10000, 784)\n",
      "(After) x_train has - min: -0.8508020140306101, max: 7.770145734421186\n",
      "(After) x_test has - min: -0.731905259880747, max: 7.57798901103678\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data(\n",
    "    path=os.path.join(os.getcwd(), 'data', 'mnist.npz')\n",
    ")\n",
    "\n",
    "print(f\"(Before) x_train has shape: {x_train.shape}\")\n",
    "print(f\"(Before) x_test has shape: {x_test.shape}\")\n",
    "\n",
    "print(f\"(Before) x_train has - min: {np.min(x_train)}, max: {np.max(x_train)}\")\n",
    "print(f\"(Before) x_test has - min: {np.min(x_test)}, max: {np.max(x_test)}\")\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], -1)\n",
    "x_test = x_test.reshape(x_test.shape[0], -1)\n",
    "\n",
    "print(f\"(After) x_train has shape: {x_train.shape}\")\n",
    "print(f\"(After) x_test has shape: {x_test.shape}\")\n",
    "\n",
    "x_train = (x_train - np.mean(x_train, axis=1, keepdims=True)) / np.std(x_train, axis=1, keepdims=True)\n",
    "x_test = (x_test - np.mean(x_test, axis=1, keepdims=True)) / np.std(x_test, axis=1, keepdims=True)\n",
    "\n",
    "print(f\"(After) x_train has - min: {np.min(x_train)}, max: {np.max(x_train)}\")\n",
    "print(f\"(After) x_test has - min: {np.min(x_test)}, max: {np.max(x_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "573a3f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "480/480 [==============================] - 8s 14ms/step - loss: 0.2310 - acc: 0.9287 - val_loss: 0.1186 - val_acc: 0.9643\n",
      "Epoch 2/10\n",
      "480/480 [==============================] - 7s 15ms/step - loss: 0.0933 - acc: 0.9719 - val_loss: 0.1101 - val_acc: 0.9682\n",
      "Epoch 3/10\n",
      "480/480 [==============================] - 6s 12ms/step - loss: 0.0655 - acc: 0.9801 - val_loss: 0.1158 - val_acc: 0.9706\n",
      "Epoch 4/10\n",
      "480/480 [==============================] - 6s 12ms/step - loss: 0.0482 - acc: 0.9850 - val_loss: 0.1172 - val_acc: 0.9731\n",
      "Epoch 5/10\n",
      "480/480 [==============================] - 6s 13ms/step - loss: 0.0403 - acc: 0.9881 - val_loss: 0.1234 - val_acc: 0.9749\n",
      "Epoch 6/10\n",
      "480/480 [==============================] - 7s 15ms/step - loss: 0.0316 - acc: 0.9902 - val_loss: 0.1399 - val_acc: 0.9757\n",
      "Epoch 7/10\n",
      "480/480 [==============================] - 6s 13ms/step - loss: 0.0286 - acc: 0.9915 - val_loss: 0.1342 - val_acc: 0.9751\n",
      "Epoch 8/10\n",
      "480/480 [==============================] - 6s 13ms/step - loss: 0.0251 - acc: 0.9933 - val_loss: 0.1471 - val_acc: 0.9760\n",
      "Epoch 9/10\n",
      "480/480 [==============================] - 6s 13ms/step - loss: 0.0212 - acc: 0.9942 - val_loss: 0.1487 - val_acc: 0.9772\n",
      "Epoch 10/10\n",
      "480/480 [==============================] - 6s 12ms/step - loss: 0.0200 - acc: 0.9942 - val_loss: 0.1514 - val_acc: 0.9790\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 0.1527 - acc: 0.9796\n",
      "Testing Results: \n",
      "\tLoss: 0.15269635617733002\n",
      "\tAccuracy: 0.9796000123023987\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "img_width, img_height = 28,28\n",
    "input_size = img_height * img_width\n",
    "num_labels = 10\n",
    "\n",
    "def mnist_model():\n",
    "    \"\"\" Defining the model using Keras sequential API \"\"\"\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(500, activation='relu'),\n",
    "        tf.keras.layers.Dense(250, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Defining the model, optimizer and a loss function\n",
    "model = mnist_model()\n",
    "optimizer = tf.keras.optimizers.RMSprop()\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "model.compile(optimizer=optimizer, loss=loss_fn, metrics=['acc'])\n",
    "\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "\"\"\" Model training \"\"\"    \n",
    "\n",
    "# Creating onehot encoded labels\n",
    "y_onehot_train = np.zeros((y_train.shape[0], num_labels), dtype=np.float32)\n",
    "y_onehot_train[np.arange(y_train.shape[0]), y_train] = 1.0\n",
    "\n",
    "# Training Phase \n",
    "train_history = model.fit(x_train,y_onehot_train, batch_size=batch_size, epochs=NUM_EPOCHS, validation_split=0.2)\n",
    "\n",
    "\"\"\" Testing phase \"\"\"\n",
    "\n",
    "# Test inputs and targets\n",
    "y_onehot_test = np.zeros((y_test.shape[0], num_labels), dtype=np.float32)\n",
    "y_onehot_test[np.arange(y_test.shape[0]), y_test] = 1.0\n",
    "\n",
    "# Evaulte on test data\n",
    "test_res = model.evaluate(x_test, y_onehot_test, batch_size=batch_size)\n",
    "\n",
    "print(\"Testing Results: \")\n",
    "print(f\"\\tLoss: {test_res[0]}\")\n",
    "print(f\"\\tAccuracy: {test_res[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe1600f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
